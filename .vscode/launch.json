{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python: Eval - MathVista - (GPT4) evaluation.generate_response",
            "type": "debugpy",
            "request": "launch",
            "module": "evaluation.generate_response",
            "justMyCode": true,
            "args": [
                "--max_num_problems",
                "20",
                "--output_dir",
                "_results/eval/mathvista/gpt4/debug",
                "--output_file",
                "gpt4.json",
            ],
            "envFile": "${workspaceFolder}/.env",
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                "LOGLEVEL": "DEBUG",
            },
        },
        {
            "name": "Python: Eval - MathVista - (GPT4) evaluation.extract_answer",
            "type": "debugpy",
            "request": "launch",
            "module": "evaluation.extract_answer",
            "justMyCode": true,
            "args": [
                "--results_file_path",
                "_results/eval/mathvista/gpt4/debug/gpt4.json",
            ],
            "envFile": "${workspaceFolder}/.env",
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                "LOGLEVEL": "DEBUG",
            },
        },
        {
            "name": "Python: Eval - MathVista - (GPT4) evaluation.calculate_score",
            "type": "debugpy",
            "request": "launch",
            "module": "evaluation.calculate_score",
            "justMyCode": true,
            "args": [
                "--output_dir",
                "_results/eval/mathvista/gpt4/debug",
                "--output_file",
                "gpt4.json",
                "--score_file",
                "gpt4_metric.json",
            ],
            "envFile": "${workspaceFolder}/.env",
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                "LOGLEVEL": "DEBUG",
            },
        },
        {
            "name": "Python: Eval - MathVista - (LLaVA) evaluation.generate_response",
            "type": "debugpy",
            "request": "launch",
            "module": "evaluation.generate_response",
            "justMyCode": true,
            "args": [
                "--conv-mode",
                "vicuna_v1",
                "--max_num_problems",
                "100",
                "--output_dir",
                "_results/eval/mathvista/llava/debug",
                "--output_file",
                "llava-v1.5-7b.json",
                "--model_path",
                "liuhaotian/llava-v1.5-7b",
                // TODO: Find out why loading from folder during debug not work?
                // "/mnt/mattmprojects/exp/projects/mattm-projectwillow/amlt-results/7293878818.78845-fe8da8ef-7f38-4b40-b2e1-3259c7bf7a3e/llava/checkpoints/llava-vicuna-7b-v1.5-finetune",
                "--save_every",
                "1",
            ],
            "envFile": "${workspaceFolder}/.env",
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                "LOGLEVEL": "DEBUG",
            },
        },
        {
            "name": "Python: Eval - MathVista - (LLaVA) evaluation.extract_answer",
            "type": "debugpy",
            "request": "launch",
            "module": "evaluation.extract_answer",
            "justMyCode": true,
            "args": [
                "--max_num_problems",
                "100",
                "--results_file_path",
                "_results/eval/mathvista/llava/debug/llava-v1.5-7b.json",
                // "${workspaceFolder}/_results/eval/mathvista/20240213_223832/llava-v1.5-7b.json",
                // "${workspaceFolder}/_results/eval/mathvista/debug/output_gpt4_2shot_solution_use_caption_ocr.json",
            ],
            "envFile": "${workspaceFolder}/.env",
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                "LOGLEVEL": "DEBUG",
            },
        },
        {
            "name": "Python: Eval - MathVista - (LLaVA) evaluation.calculate_score",
            "type": "debugpy",
            "request": "launch",
            "module": "evaluation.calculate_score",
            "justMyCode": true,
            "args": [
                "--output_dir",
                // All Empty Responses
                // "_results/eval/mathvista/20240214_210722",
                // Legitimate Responses
                // "_results/eval/mathvista/20240214_220834",
                "_results/eval/mathvista/20240215_204602",
                "--output_file",
                "llava-v1.5-7b.json",
                // "llava-v1.5-7b_false_positives.json",
                "--score_file",
                "llava-v1.5-7b_metrics.json",
                "--ignore_empty_extractions",
            ],
            "envFile": "${workspaceFolder}/.env",
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                "LOGLEVEL": "DEBUG",
            },
        },
    ]
}
